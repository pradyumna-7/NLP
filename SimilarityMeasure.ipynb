{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cosine Similarity with TF - IDF** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "Term Frequency (TF) measures how often a term appears in a document. It is calculated as:\n",
    "\n",
    "$$\n",
    "TF(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f_{t,d}$ is the number of times term $t$ appears in document $d$,\n",
    "- $\\sum_{t' \\in d} f_{t',d}$ is the total number of terms in document $d$.\n",
    "\n",
    "**Example:**  \n",
    "If a document contains **100 words**, and the word \"data\" appears **5 times**, then:\n",
    "\n",
    "$$\n",
    "TF(\\text{\"data\"}) = \\frac{5}{100} = 0.05\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "Inverse Document Frequency (IDF) measures how important a term is by considering how many documents contain it. It is given by:\n",
    "\n",
    "$$\n",
    "IDF(t) = \\log \\frac{N}{DF(t)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( N \\) is the total number of documents,\n",
    "- \\( DF(t) \\) is the number of documents that contain term \\( t \\).\n",
    "\n",
    "**Example:**  \n",
    "If there are **10,000 documents** and \"data\" appears in **1,000** of them, then:\n",
    "\n",
    "$$\n",
    "IDF(\\text{\"data\"}) = \\log \\frac{10000}{1000} = \\log 10 = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF Formula\n",
    "TF-IDF is computed as:\n",
    "\n",
    "$$\n",
    "TF-IDF(t, d) = TF(t,d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "Using our previous examples:\n",
    "\n",
    "$$\n",
    "TF-IDF(\\text{\"data\"}) = 0.05 \\times 1 = 0.05\n",
    "$$\n",
    "\n",
    "Thus, the **TF-IDF score for \"data\" in this document is 0.05**, indicating its importance relative to other terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores for Document 1:\n",
      "  and: 0.0000\n",
      "  artificial: 0.3541\n",
      "  branch: 0.0000\n",
      "  deep: 0.0000\n",
      "  field: 0.4977\n",
      "  intelligence: 0.3541\n",
      "  is: 0.3541\n",
      "  learning: 0.3541\n",
      "  machine: 0.3541\n",
      "  of: 0.3541\n",
      "\n",
      "TF-IDF scores for Document 2:\n",
      "  and: 0.3638\n",
      "  artificial: 0.2588\n",
      "  branch: 0.3638\n",
      "  deep: 0.3638\n",
      "  field: 0.0000\n",
      "  intelligence: 0.2588\n",
      "  is: 0.2588\n",
      "  learning: 0.5177\n",
      "  machine: 0.2588\n",
      "  of: 0.2588\n",
      "\n",
      "Vector representation of Document 1:\n",
      "[0.         0.35409974 0.         0.         0.49767483 0.35409974\n",
      " 0.35409974 0.35409974 0.35409974 0.35409974]\n",
      "\n",
      "Vector representation of Document 2:\n",
      "[0.36378803 0.25883818 0.36378803 0.36378803 0.         0.25883818\n",
      " 0.25883818 0.51767635 0.25883818 0.25883818]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Sample texts\n",
    "text1 = \"Machine learning is a field of artificial intelligence.\"\n",
    "text2 = \"Deep learning is a branch of artificial intelligence and machine learning.\"\n",
    "# Convert texts to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create dictionaries with TF-IDF scores for each document\n",
    "tfidf_scores_doc1 = dict(zip(feature_names, tfidf_array[0]))\n",
    "tfidf_scores_doc2 = dict(zip(feature_names, tfidf_array[1]))\n",
    "\n",
    "# Print TF-IDF scores for each word in each document\n",
    "print(\"TF-IDF scores for Document 1:\")\n",
    "for word, score in sorted(tfidf_scores_doc1.items()):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTF-IDF scores for Document 2:\")\n",
    "for word, score in sorted(tfidf_scores_doc2.items()):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Print vector representation of each document\n",
    "print(\"\\nVector representation of Document 1:\")\n",
    "print(tfidf_array[0])\n",
    "\n",
    "print(\"\\nVector representation of Document 2:\")\n",
    "print(tfidf_array[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (TF-IDF): 0.6416\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "print(f\"Cosine Similarity (TF-IDF): {cosine_sim[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cosine Similarity with Word2Vec - GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "Cosine Similarity (Word Embeddings - GloVe): 0.9705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load pretrained GloVe model\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "def get_embedding(text, model):\n",
    "    words = text.lower().split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "# Compute embeddings\n",
    "embedding1 = get_embedding(text1, glove_model)\n",
    "embedding2 = get_embedding(text2, glove_model)\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity([embedding1], [embedding2])\n",
    "print(f\"Cosine Similarity (Word Embeddings - GloVe): {cosine_sim[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (Word Embeddings - spaCy): 0.9144\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy's pre-trained model with word vectors (small model for speed)\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Use \"en_core_web_lg\" for larger vectors\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Compute the document embedding by averaging word vectors.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return doc.vector  # SpaCy provides a mean-pooled vector representation\n",
    "\n",
    "# Example texts\n",
    "text1 = \"Machine learning is fascinating.\"\n",
    "text2 = \"Deep learning is a subset of machine learning.\"\n",
    "\n",
    "# Compute embeddings\n",
    "embedding1 = get_embedding(text1)\n",
    "embedding2 = get_embedding(text2)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "print(f\"Cosine Similarity (Word Embeddings - spaCy): {cosine_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Cosine Similarity**\n",
    "Cosine Similarity measures the cosine of the angle between two vectors, indicating how similar they are in direction.\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $A \\cdot B$ is the dot product of vectors $A$ and $B$,\n",
    "- $\\|A\\|$ and $\\|B\\|$ are the magnitudes (norms) of the vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Euclidean Distance**\n",
    "Euclidean Distance measures the straight-line distance between two points in space.\n",
    "\n",
    "$$\n",
    "d(A, B) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $A_i$ and $B_i$ are the components of vectors $A$ and $B$.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Manhattan Distance**\n",
    "Manhattan Distance (L1 norm) measures the absolute sum of differences between two vectors.\n",
    "\n",
    "$$\n",
    "d(A, B) = \\sum_{i=1}^{n} |A_i - B_i|\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The difference is measured along each dimension separately, like city blocks.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Jaccard Similarity**\n",
    "Jaccard Similarity measures the ratio of the intersection over the union of two sets.\n",
    "\n",
    "$$\n",
    "J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $|A \\cap B|$ is the number of common elements,\n",
    "- $|A \\cup B|$ is the total number of unique elements.\n",
    "\n",
    "For continuous vectors, Jaccard similarity can be computed by treating nonzero elements as presence indicators.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Pearson Correlation Coefficient**\n",
    "Pearson Correlation measures the linear relationship between two vectors.\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^{n} (A_i - \\bar{A})(B_i - \\bar{B})}{\\sqrt{\\sum_{i=1}^{n} (A_i - \\bar{A})^2} \\sqrt{\\sum_{i=1}^{n} (B_i - \\bar{B})^2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\bar{A}$ and $\\bar{B}$ are the means of vectors $A$ and $B$.\n",
    "\n",
    "---\n",
    "\n",
    "These formulas provide different ways to measure similarity or distance between two vectors. The choice of measure depends on the specific application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9476\n",
      "Euclidean Distance: 1.3273\n",
      "Manhattan Distance: 7.7810\n",
      "Jaccard Similarity: 0.7576\n",
      "Pearson Correlation: 0.9466\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean, cityblock, jaccard\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load pretrained GloVe model\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "def get_embedding(text, model):\n",
    "    \"\"\"Compute the document embedding by averaging word vectors.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no words found\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"Machine learning is fascinating.\"\n",
    "text2 = \"Deep learning is a subset of machine learning.\"\n",
    "\n",
    "# Compute embeddings\n",
    "embedding1 = get_embedding(text1, glove_model)\n",
    "embedding2 = get_embedding(text2, glove_model)\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "      \n",
    "# Compute Euclidean Distance\n",
    "euclidean_dist = euclidean(embedding1, embedding2)\n",
    "\n",
    "# Compute Manhattan Distance\n",
    "manhattan_dist = cityblock(embedding1, embedding2)\n",
    "\n",
    "# Compute Jaccard Similarity (using binary vectors)\n",
    "jaccard_sim = jaccard(embedding1 > 0, embedding2 > 0)  # Convert to binary presence/absence\n",
    "\n",
    "# Compute Pearson Correlation\n",
    "pearson_corr, _ = pearsonr(embedding1, embedding2)\n",
    "\n",
    "# Display Results\n",
    "print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_dist:.4f}\")\n",
    "print(f\"Manhattan Distance: {manhattan_dist:.4f}\")\n",
    "print(f\"Jaccard Similarity: {1 - jaccard_sim:.4f}\")  # Convert distance to similarity\n",
    "print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
